--------------------------------------------------------------------------------
EVENTQL - PRE-AGGREGATION
--------------------------------------------------------------------------------
v0.1 - September, 2016                             Paul Asmuth <paul@eventql.io>

Table of Contents

  1. Introduction
  2. Definitions
  3. Design Overview
  4. Implementation Details
    4.1 Configuration
    4.2 Pre-Aggregation Procedure
    4.3 Leader Election
    4.4 Load Balancing
    4.5 Changes to the INSERT statement
    4.6 Changes to the SELECT statement
  5. Reliability Considerations
  6. Alternatives Considered
    6.1 Implement as standalone service
    6.2 Implement in the compaction routine
  7. Code Locations


1. Introduction

  One of the primary usecases of EventQL is multidimensional analysis of
  timeseries data. These type of analyses usually involve a table that is
  indexed by a DATETIME primary key and contains a number of other columns that
  can be classified into "dimensions" and "measurements".

  Dimensions are columns that may be referred in the GROUP BY statement of
  queries on the data. Measurements are (usually numeric) columns that will only
  be used in aggregations (i.e. from aggregate function in the SELECT list).

  For this usecase it's usually highly beneficial to pre-aggregate the
  measurements for each input dimension over a recurring time interval before
  writing them to disk.

  To see why this may result in huge performance increases, consider this
  somewhat realistic usecase: We're running an ad network and are displaying
  100,000 ad impressions per second. We want to measure the rate at which these
  ads are clicked with 1-minute granularity. To make things a bit more
  interesting, we also want to be able to get an individual click rate for each
  of the websites on which our ads are displayed.

  If our ads are displayed on 4,000 individual websites at any given time, the
  examples works out so that the size of the data after pre-aggreagtion would be
  roughly 0.07% of the size of the input data. A 1500x speedup.


2. Definitions

  We define an abstract pre-aggregation routine, that accepts a rowset N of
  rows for a given input time interval and produces another rowset M, so that
  any query that computes aggregations on the measurements (and optionally
  groups by time or one of the dimensions) will return the same result for both
  input rowsets N and M. Note that the number of rows in M will always be less
  or equal to the number of rows in N.

  The length of the recurring aggregation time interval represents a lower bound
  on the time granularity over which the data can be grouped in queries. Hence
  we'll refer to length of an individual aggregation time interval as the
  "granularity" of the aggregation from now on.

  We also define the "cardinality of an aggregation" for a given time interval
  as the number of rows in the output rowset M. Given an arrival rate of events
  R and a granularity G, the pre-aggregation function transforms R * G input
  rows into C output rows where C is the cardinality of the aggregation.


3. Design Overview

  The proposed approach is to implement pre-aggregation as a separate internal
  subsystem, touching as few of the core subsystems as possible.

  Pre-aggregation is optionally enabled for a table by setting a pre-aggregation
  config for the table (further specified in section 4).

  Once enabled, we elect N hosts as "aggregation nodes" for the table. Each
  insert into the pre-aggregation-enabled table will bypass the normal insert
  code and will instead be sent to one of the N aggregation hosts. Each
  aggregation node then performs the pre-aggreagtion routine locally on all
  received input rows and, at the end of the aggregation interval, forwards the
  output rows to the regular table insert procedure.

  Of course, this scheme will result in up to N * C instead of C output rows.
  While this would not impact the correctness of any aggregate queries over the
  data, it would still be visible to the user when doing a simple select on the
  table. For UX reasons, and because we consider it an implementation detail
  that should not have to be explained in user-facing documentation, we
  automatically insert a default aggregation clause into every SELECT select
  statement that refers to pre-aggregation-enabled tables. This default
  aggregation clause is statically derived from the pre-aggregation config.


4. Implementation Details

4.1 Configuration

  The pre-aggregation config may be changed at anye time.

4.2 Pre-Aggregation Procedure

4.3 Leader Election

4.4 Load Balancing

4.5 Changes to the INSERT statement

4.6 Changes to the SELECT statement


5. Relability Considerations

  The proposed implementation has no provisions to handle pending data on server
  shutdown or server crashes, so in these events some input rows will be lost.
  As part of the separate writea-ahead-logging effort, we will add (optional)
  durable write-ahead storage for accepted input rows.

  A separate issue that is not currently addressed is that the current API
  and implementation do not allow for exactly-once-inserts in the case of
  failures: If we receive an error code after posting an insert to one of the
  aggregating servers, we can't be sure if the value was stored or not and any
  retries would not be idempotent. Still, in practice it is much more likely
  that the value was _not_ inserted. For now we'll do a limited number of
  retries and consider the pre-aggregation an at-least-once system.

  At a later point, we will solve this by adding an optional transaction IDs to
  every insert and ignoring duplicate inserts, however this would be done in an
  independent effort as a similar issue exists for inserts into regular (i.e.
  not pre-aggregation-enabled) tables with a non unique primary key.


6. Alternatives Considered

6.1 Implement as standalone service

  On first look, pre-aggregation might seem like a feature that does not belong
  in the core database, but could be provided as a separate service that
  performs the pre-aggregation and then simply stores the aggregated rows into
  the database.

  However, with this scheme it would be painful to support more complex
  aggregation modes, like estimating the number of unique strings or estimating
  quantiles.

  Both of these problems would be easily solvable using probabilistic data
  structures, but if the pre-aggregation was performed in a standalone service
  we would have to precompute and serialize these data structures before sending
  them to the database. And we can't treat the data as binary blobs in the
  database if we want to run useful (and fast) queries on it, so this
  would leave us with one of two choices:

  Either duplicate all data strcture code from the database into the standalone
  service or a fat client and then make sure that both copies always match 100%
  so the serialization works (which would be a PITA). Or alternatively, keep
  the code only in the database and send all the raw values to the database as
  part of the aggregated row (which would defeat the purpose of streaming
  aggregation).

6.2 Implement in the compaction routine

  One downside of the proposed approach is that with constant arrival rate R and
  granularity G, the number of stored rows for a given input stream still scales
  linearly in the number of hosts we assign to the stream (and therefore,
  strictly speaking also in the arrival rate R).

  One alternative solution, implementing the pre-aggregation in the partition
  compaction routine, avoids this and ends up with a constant number of stored
  rows regardless of the number of servers assigned to the aggregation.

  However, in the interest of keeping the design simple and minimally invasive
  to the other subsystems, modifying the compaction routine is left open for
  when/if the issue turns out to be practical limitation.


7. Code Locations

  FIXME

