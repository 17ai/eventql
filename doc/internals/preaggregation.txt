--------------------------------------------------------------------------------
EVENTQL - PRE-AGGREGATION
--------------------------------------------------------------------------------
v0.1 - September, 2016                             Paul Asmuth <paul@eventql.io>

Table of Contents

  1. Introduction
  2. Design Overview
  3. Code Locations


1. Introduction

  One of the primary usecases of EventQL is multidimensional analysis of
  timeseries data. These type of analyses usually involve a table that is
  indexed by a DATETIME primary key and contains a number of other columns that
  can be classified into "dimensions" and "measurements".

  Dimensions are columns that may be referred in the GROUP BY statement of
  queries on the data. Measurements are (usually numeric) columns that will only
  be used in aggregations (i.e. from aggregate function in the SELECT list).

  For this usecase it's highly beneficial to pre-aggregate the measurements for
  each input dimension over a recurring time interval before writing them to
  disk.

  The pre-aggregation routine conceptually accepts a rowset N and produces
  another rowset M, so that any query that computes aggregations on the
  measurements (and optionally groups by the dimensions) will return the same
  result for both input rowsets N and M. Note that the number of rows in M will
  always be less or equal to the number of rows in N.

  The length of the recurring time interval represents a lower bound on the
  (time) granularity over which the data can be grouped in queries. Hence we'll
  refer to length of an individual aggregation time interval as the
  "granularity" of the aggregation from now on.

  We also define the "cardinality of an aggregation" for a given time interval
  as the number of rows in the output rowset M. Given an arrival rate of events
  R and a granularity G, the pre-aggregation function transforms R * G input
  rows into C output rows where C is the cardinality of the aggregation.

  To see why this may result in huge performance increases, consider this
  somewhat realistic usecase: We're running an ad network and are displaying
  100,000 ad impressions per second. We want to measure the rate at which these
  ads are clicked with 1-minute granularity. To make things a bit more
  interesting, we also want to be able to get an individual click rate for each
  of the websites on which our ads are displayed.

  So, let's say our ads are displayed on 4,000 individual websites at any given
  time. That would make C=4,000, R=100,000 impressions/second and G=1 minute.
  This works out so that the size of the data after pre-aggreagtion would be
  roughly 0.07% of the size of the input data. A 1500x speedup.

  On first look, pre-aggregation might seem like a feature that does not belong
  in the core database, but could be provided as a separate service that
  performs the pre-aggregation and then simply stores the aggregated rows into
  the database.


1. Design Overview

  Considering that the pre-aggregation feature should "almost" have been a
  standalone service and not a feature, the proposed approach is to implement
  it as a separate internal subsystem, touching as few of the core subsystems
  as possible.

  Pre-aggregation is optinally enabeld for a table by setting a pre-aggregation
  config for the table (further specified in section 4).

  Once enabled, we elect N hosts as "aggregation nodes" for the table. Each
  insert into the pre-aggregation-enabled table will bypass the normal insert
  code and will instead be sent randomly to one of the N aggregation hosts
  (N.B. actual load balancing might not be random). Each aggregation node then
  performs the pre-aggreagtion routine locally on all received input rows and,
  at the end of the aggregation interval, forwards the output rows to the
  regular table insert procedure.

  Of course, this scheme will result in up to N * C instead of C output rows.
  While this would not impact the correctness of any aggregate queries over the
  data, it would still be visible to the user when doing a simple select on the
  table. For UX reasons, and because we consider it an implementation detail
  that should not have to be explained in user-facing documentation, we
  automatically insert a default aggregation into every SELECT. The default
  aggregation is derieved from the pre-aggregation config.

3. Code Locations

  FIXME

4. Alternatives Considered


